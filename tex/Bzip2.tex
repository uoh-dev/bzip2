\documentclass{article}
%\usepackage[margin=1.3in]{geometry}
\usepackage[margin=1.84in, top=4cm]{geometry}

\date{Abgabe: \today}
\author{Florian Brohm, 7443251 \and Toprak Saricerci, 7445073}

\title{Seminar: Datenkompression\\Bzip2}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{wasysym}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[ngerman]{babel}
%\usepackage{changepage}
%\usepackage[thinc]{esdiff}
%\usepackage[makeroom]{cancel}
%\usepackage{pdfpages}

%\usepackage{tikz}
\usepackage{listings}
%\usepackage{tabularx}
\usepackage{float}
\usepackage[font=small]{caption}

%\usetikzlibrary{arrows,automata}

\usepackage[sorting=none]{biblatex}
\addbibresource{quellen.bib}

\newcommand{\bla}{\bigwedge\limits}
\newcommand{\blo}{\bigvee\limits}
\newcommand{\la}{\land}
\newcommand{\lo}{\lor}
\newcommand{\lr}{\rightarrow}
\newcommand{\llr}{\leftrightarrow}
\newcommand{\Llr}{\Leftrightarrow}
\newcommand{\lp}{\oplus}
\newcommand{\p}{\text{potenz}}
\newcommand{\R}{\Rightarrow}
\newcommand{\n}[1]{\overline{{#1}}}
\newcommand{\bb}[1]{\mathbb{{#1}}}

\newcommand{\step}[1]{&& \left|\ {#1} \right.}
\newcommand{\e}[2]{{#1}\cdot 10^{{#2}}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\red}{\textcolor{red}}
\newcommand{\quelle}{\red{quelle }}
\newcommand{\todo}{\red{todo }}
\newcommand{\dunderline}[1]{\underline{\underline{#1}}}
\newcommand{\reff}[2]{\hyperref[{#2}]{{#1}\ref*{#2}}}

\newcommand{\para}{\par\null\par}
\setlength\parindent{0pt}
\lstset{mathescape=true}

\usepackage{hyperref}
\hypersetup{
	colorlinks,
	linkcolor={blue},
	urlcolor={red}, 
    citecolor={blue}
}

\begin{document}
\maketitle
\newpage

{
    \hypersetup{linkcolor=blue}
    \tableofcontents
}
\listoffigures
\newpage
\section{Was ist Bzip2?}
\vspace*{3cm}
\section{Was ist eine Datenkompression?}
Als Datenkompression wird ein Vorgang bezeichnet, bei dem Daten so verarbeitet werden, 
dass sie mit weniger Bits darstellbar sind. Dabei kann die Kompression entweder 
\texttt{verlustfrei} oder \texttt{verlustbehaftet} sein.
\subsection{Verlustfreie und verlustbehaftete Kompressionen}
Für eine Kompression $k: E \to A$, die eine Eingabe $E \in \{0,1\}^*$ in eine Ausgabe $A \in \{0,1\}^*$
komprimiert, gilt:
\[k \text{ ist \texttt{verlustfrei}} \Llr k \text{ ist invertierbar}.\]
Bei \texttt{verlustfreien} Kompressionen wird also die Forderung gesetzt, dass die 
komprimierte Ausgabe wieder in die originale Eingabe dekomprimiert werden~kann.
\para
Dass eine Kompression verlustbehaftet ist, ist nicht immer schlimm. Wichtig ist hierbei nur, 
dass die verlorene Information für den Menschen nicht erkennbar oder zumindest 
vernachlässigbar ist. So kann zum Beispiel bei einem Bild das Farbspektrum 
verkleinert werden, ohne dass das menschliche Auge einen Unterschied bemerkt.
\begin{figure}[H]
    \centering
    \includegraphics*[width=0.5\textwidth]{images/jpeg.jpg}
    \caption[Verlustbehaftete Kompression eines Bildes]{
        Ein Bild, welches mit dem mit dem verlustbehafteten JPEG-Verfahren komprimiert~wurde (rechts), 
        sodass Kompressionsartefakte zu erkennen sind \cite*{jpeg}.
    }
\end{figure}
In Texten hingegen können schon kleine Informationsverluste dafür sorgen, 
dass diese nicht mehr lesbar sind.
\subsection{Entropiekodierungen und Substitiutionskompressionen}
Bei verlustfreien Kompressionen unterscheidet man grundsätzlich zwischen den
\texttt{Entropiekodierungen} und \texttt{Substitiutionskompressionen}.
\para
\texttt{Substitiutionskompressionen} versuchen, oft wiederholende Zeichenfolgen 
zusammenzufassen oder Muster und somit Redundanz in der Eingabe zu eliminieren. Bekannte Vertreter
sind hier das LZ77 Verfahren und das im folgenden Abschnit analysierte Run-Length Verfahren.
\para
\texttt{Entropiekodierungen} arbeiten im Gegensatz zu Substitiutionskompressionen nicht 
mit Zeichenfolgen oder Mustern in der Eingabe, sondern mit den relativen Häufigkeiten
der einzelnen Zeichen. Dabei ist das Ziel, dass häufig auftretende Zeichen weniger
Bits zur Folge haben und seltene Zeichen mehr Bits.
Die Huffman -und arithmetische Kodierung sind Beispiele von Entropiekodierungen.
\section{Run-Length encoding} 
Die Run-Length Kodierung ist eine Substitiutionskompression, bei der sich wiederholende
Zeichenfolgen zusammengefasst werden.
\subsection{Kodierung}
Sei $aw \in \Sigma^*$ die Eingabe, wobei $a = x^n,~x\in \Sigma$ eine
sich wiederholende Zeichenfolge der Länge $n$ ist. Das Teilwort $a$ wird zu $nx$ umgeformt, 
wonach der Algorithmus mit $w$ fortführt. Folgendes ist ein Beispiel der Kodierung:
\begin{center}
    \red{jjjjjjj}kkijjj $\R$ 7j\red{kk}ijjj $\R$ 7j2k\red{i}jjj $\R$ 7j2k1i\red{jjj} $\R$ 7j2k1i3j
\end{center}
\subsection{Dekodierung}
Sei $nxw$ mit $n\in \bb{N},~x\in\Sigma$ und $w\in(\bb{N}\cup\Sigma)^*$ die Eingabe. Das Teilwort $nx$ wird
zu $x^n$ umgeformt, wonach der Algorithmus mit $w$ fortführt. Folgendes ist ein Beispiel der Kodierung:
\begin{center}
    \red{2j}1k6i3k $\R$ jj\red{1k}6i3k $\R$ jjk\red{6i}3k $\R$ jjkiiiiii\red{3k} $\R$ jjkiiiiiikkk
\end{center}
\subsection{Effizienz}
Solange die Eingabe viele sich wiederholende Zeichenfolgen enthält, ist die Run-Length 
Kodierung sehr effektiv. Falls aber nicht, kann die Kompression sogar zu einem größeren 
Platzverbrauch führen. 
Die Zeichenfolge \textsf{abab}$\dots$ würde zum Beispiel zu \textsf{1a1b1a1b}$\dots$ komprimiert werden. 
Falls sowohl Counter als auch Zeichen genau ein Byte Platz bräuchten (kommt ganz auf die
Implementierung an), würde die Kompression zu einer Verdopplung der Bits führen.
\subsection{Implementierung}
Eine einfache und effiziente Implementierung von Run-Length arbeitet mit 8-Bit Zeichen, meist \texttt{ASCII},
und hat für den Counter eine Obergrenze von 255, also auch 8-Bit. Das vereinfacht das 
Dekodieren deutlich, da Counter von Zeichen anhand der Bitposition des Textes unterschieden
werden kann. Wenn das Byte auf einem ungeraden Index ist, handelt es sich bei diesem 
um einen Counterbyte und ansonsten um ein Zeichenbyte.
\begin{figure}[H]
    \centering
    \includegraphics*[width=0.8\textwidth]{images/bytesteps.png}
    \caption[Counter -und Zeichenbytes beim Run-Length]{
        Counter -und Zeichenbytes beim Run-Length nach obiger Variante
    }
\end{figure}

Außerdem gibt es eine Variante von Run-Length, die zum Beispiel nur Zeichenwiederholungen der Länge 4 oder größer 
zusammenfasst, um die mögliche Verdopplung des Platzverbrauchs zu verhindern. 
\textsf{abab}$\dots$ würde dann nicht verändert werden, da alle Zeichenwiederholungen die Länge 1 haben.
So ist es aber nicht mehr einfach möglich, Counter von Zeichen zu unterscheiden.
\subsection{Pseudocode}
\subsubsection{Kodierung}
\lstinputlisting{../presentation/pseudo/run-length.pseudo}
\subsubsection{Dekodierung}
\lstinputlisting{../presentation/pseudo/run-length-back.pseudo}
\newpage
\section{Huffman encoding}
Die Huffman-Kodierung errechnet für Zeichen $z \in \Sigma$ einer Eingabe einen Code 
\[c: \Sigma \to \bigcup\limits_{n\geq1} \{0,1\}^n.\]
Dafür zählt die Kodierung die relative Häufigkeit jedes Zeichens und 
gibt Zeichen mit hoher relativer Häufigkeit kürzere Codes und Zeichen mit kleinerer relativer Häufigkeit
entsprechend längere Kodes. Bei Eingaben mit ungleichen Häufigkeitsverteilungen machen
einige wenige Zeichen den Großteil der Eingabe aus, sodass ein Großteil
der Zeichen kurze Codes hat, was eine kleine durchschnittliche Codelänge und
somit große Platzeinsparungen zur Folge hat.
\subsection{Präfixfreiheit}
Ein Code $c: \Sigma \to \Gamma$ gilt dann als \texttt{präfixfrei}, wenn
\[\forall x, y \in\textsf{Bild}(c) : \nexists z \in\Gamma : xz = y.\] 
Präfixfreie Codes wie bei der Huffman-Kodierung haben den Vorteil, 
man so aus einer Codefolge direkt die 
originale Eingabe linear errechnen kann. Eine Bitfolge ist genau dann ein Zeichen 
der originalen Eingabe, wenn die Bitfolge im Bild von $c$ ist, welche bei der 
Huffman-Kodierung aus der gespeicherten Tabelle berechnet werden können.
\subsection{Kodierung}
Die Kodierung besteht aus folgenden Schritten:
\begin{enumerate}
    \item Zähl in der Eingabe, wie oft jedes Zeichen vorkommt und berechne so die relative Häufigkeit.
    \item Erstelle einen Graph, wobei jedes Zeichen einen Knoten hat, indem seine relative Häufigkeit und sein Zeichen gespeichert sind.
    \item Solange es mehr als einen Knoten gibt, der kein Elternknoten besitzt: 
        \begin{itemize}
            \item Erstelle einen Knoten $k$, dessen beiden Kinder die Knoten im Graph mit der 
            kleinsten relativen Häufigkeit sind.
            \item Die linke Kante wird mit 0 und die rechte mit 1 markiert.
            \item Setze die relative Häufigkeit von $k$ auf die Summe der relativen Häufigkeiten seiner beiden Kinder.
        \end{itemize} 
    \item Erstelle eine Tabelle, in der jedes Zeichen einem Code zugeordnet wird.    
    \item Traversiere den Baum von der Wurzel
        \begin{itemize}
            \item Sobald ein Blatt erreicht wird, wird dessen Zeichen der zurückgelegte Weg als Kodierung zugewiesen.
        \end{itemize}
    \item Setze jedes Zeichen der Eingabe auf seinen Code in der Tabelle.
\end{enumerate}
\subsection{Dekodierung}
Da bei der Kodierung der Eingabe auch die Tabelle mitgespeichert wird, kann man einfach den 
kodierten Text Bit für Bit durchgehen, bis die Bitfolge als Eintrag in der Tabelle zu finden ist.
Da der Code präfixfrei ist, kann es keinen anderen Code in der Tabelle geben, der mit dieser
Bitfolge erreicht werden könnte, sodass die Zuordnung eindeutig ist.
\subsection{Effizienz}
Da die Tabelle gespeichert werden muss und seine Größe linear von der Alphabetgröße abhängt,
weil jedes Zeichen im Alphabet ein Eintrag besitzt, hat die Huffman-Kodierung einen riesigen
Overhead. Bei einem Alphabet mit 256 Zeichen lohnt sich die Huffman-Kodierung für eine 
Eingabe mit weniger als 1000 Zeichen überhaupt nicht, da die Tabellengröße einen großen 
Anteil der Gesamtgröße ausmacht.
Erst bei einer sehr großen Anzahl an Zeichen, z.B. 100000 ist die Tabellengröße 
vernachlässigbar. 
\subsection{Pseudocode}
Da im 3. Schritt der Kodierung immer die Kinder mit der kleinsten relativen Häufigkeit
gesucht werden, eignet sich ein Min-Heap als Datenstruktur für eine schnelle Suche.
\subsubsection{Tabellengenerierung}
\lstinputlisting{../presentation/pseudo/huffman-table.pseudo}
\newpage
\subsubsection{Kodierung}
\lstinputlisting{../presentation/pseudo/huffman.pseudo}
\subsubsection{Dekodierung}
\lstinputlisting{../presentation/pseudo/huffman-back.pseudo}

\newpage
\section{Transformation}
\subsection{Was ist eine Transformation?}
Transformationen sind Funktionen auf einer Menge von Eingabewörtern $\Sigma^*$, welche diese auf eine, oft gleiche, Menge von Ausgabewörtern $\Sigma'^{*}$ abbildet. Die Eingabe und das Ergebnis sind dabei jedoch gleich lang.
\\[.5cm]
Sei $T$ eine Transformation mit
\begin{equation}
    T:~\Sigma^*\rightarrow\Sigma'^*,~w\mapsto T(w)
\end{equation}
Die Anwendung einer Transformation wird im folgenden für ein Wort $w\in\Sigma^*$ gekürzt beschrieben.
\begin{equation}
    w^T:=T(w)
\end{equation}
Aus der oben genannten Definition folgt
\begin{equation}
    |w^T|=|w|
\end{equation}
Beispiele simpler Transformationen:
\begin{align*}
    abcd~&\stackrel{R}{\rightarrow}~dcba~~\text{(Umkehrung)}\\
    dbca~&\stackrel{\text{sort}}{\rightarrow}~abcd~~\text{(Lexikographische Sortierung)}\\
    abcd~&\stackrel{0}{\rightarrow}~0000~~\text{(Nulling)}
\end{align*}
Man beobachte: die Permutation ist ein Spezialfall der Transformation.
\subsection{Warum sind Transformationen nützlich?}
Das Ziel der Datenkompression ist logischerweise das Einsparen von Speicherplatz.
Wenn jedoch die Länge der Ausgabe einer Transformation der Länge der Eingabe gleicht, wozu benötigen wir sie dann?
\\[.5cm]
Transformationen können benutzt werden, um Eingaben in eine für eine Kodierung (o.Ä.) günstige Form zu bringen. Dies steigert die Effizienz der Kodierung und spart somit mehr Speicherplatz.
\\[.5cm]
Wir stellen die Forderung, dass eine Transformation invertierbar sein muss, um für (verlustfreie) Datenkompression nützlich zu sein. Die Umkehrfunktion einer Transformation $T$ wird wie folgt definiert.
\[T^{-1}:~\Sigma'^*\rightarrow\Sigma^*,~w=\bigl(w^T\bigr)^{T^{-1}}\]
Jetzt gilt es nur noch, tatsächlich nützliche Transformationen zu finden und anzuwenden.


\section{Move-to-front transform}
\subsection{Erläuterung}
MTF ist eine Transformation, welche Zeichen der Eingabe auf deren Index in einer Liste ''neulich genutzter'' Zeichen abbildet.
\\[.5cm]
Sei $\Sigma$ das Eingabealphabet mit $|\Sigma|=n$. Wir initialisieren die Liste $A(1..n)$ folgendermaßen.
\begin{equation}
    A_{1..n}=(\sigma_1,..,\sigma_n),~\sigma_1,..,\sigma_n\in\Sigma~\text{paarweise Verschieden}
\end{equation}
$A$ enthält initial das gesamte Alphabet, per konvention lexikographisch sortiert.
\\[.5cm]
Wir definieren die Hilfsfunktion $t_{\text{MTF}}$:
\begin{equation}
\begin{split}
    &t_{\text{MTF}}:\Sigma^n\times\Sigma\rightarrow\Sigma^n\times\Sigma,\\
    &t_{\text{MTF}}(A,\sigma):=(A_i\circ(A\setminus A_i),i),~i=\text{Index von}~\sigma~\text{in}~A
\end{split}
\end{equation}
$t_{\text{MTF}}$ beschreibt einen Schritt des MTF-Algorithmus. Für ein Zeichen $\sigma\in\Sigma$ der Eingabe wird zunächst dessen Index in $A$ in die Ausgabe geschrieben. Anschließend wird das Zeichen vorne an $A$ gehängt.
\\[.5cm]
Mithilfe von $t_{\text{MTF}}$ lässt sich MTF nun definieren.
\begin{equation}
\begin{split}
    \text{MTF}_A:\Sigma^*\rightarrow~&\Sigma^*\\
    w^{\text{MTF}_A}:=~&i\circ w'\\
    \text{mit}~&(A',i)=t_{\text{MTF}}(A,w_1)\\
    &w'=(w_2,..,w_k)^{\text{MTF}_{A'}}
\end{split}
\end{equation}
\newpage
\subsection{Inverse Transformation}
Die Invertierung der MTF ist trivial, da gilt
\begin{equation}
    \text{MTF}_A^{-1}=\text{MTF}_A
\end{equation}
\subsection{Implementierung}
\subsubsection{Transformation, Inverse Transformation}
\lstinputlisting{../presentation/pseudo/mtf.pseudo}
\newpage
\section{Burrows-Wheeler transform}
\subsection{Definitionen}
Sei $w\in\Sigma^*$ mit $|w|=n$. Wir definieren die $i$-te Rotation von $w$ für $1\leq i\leq n$ folgendermaßen.
\begin{equation}
    w^{\lambda_i}:=(w_i,..,w_n,w_1,..,w_{i-1})
\end{equation}
Die Menge aller Rotationen von $w$ sei
\begin{equation}
    w^\lambda:=\{w^{\lambda_i}:1\leq i\leq n\}\subset S_n
\end{equation}
Ferner sei die Rotation von $w$ des Ranges $k$
\begin{equation}
    w^{\Lambda_k}:=w^{\lambda_i},~\text{sodass}~|\{w^{\lambda_j}:w^{\lambda_j}\leq_l w^{\lambda_i},~1\leq i,j\leq n\}|=k
\end{equation}
In einer lexikographisch sortierten Liste von Rotationen ist $w^{\Lambda_k}$ der $k$-te Eintrag.
\subsection{Erläuterung}
BWT ist eine Transformation, welche die durchschnittliche Run-Length in der Eingabe erhöht. Dazu wird die lexikographische Rangfolge der Zeichen in betracht gezogen.
\\[.5cm]
BWT sei für eine Eingabe $w\in\Sigma^*$ definiert als
\begin{equation}
\begin{split}
    \text{BWT}:~&\Sigma^*\rightarrow(\Sigma\cup\{\$\})^*,\\
    w^\text{BWT}:=~&(u^{\Lambda_1}_{n+1},..,u^{\Lambda_{n+1}}_{n+1})\\
    \text{mit}~&u=w\circ\$
\end{split}
\end{equation}
Aus einer sortierten Liste von Rotationen der Eingabe wird die Ausgabe jeweils aus dem letzten Zeichen jeder Rotationen gebildet. Davor wird jedoch ein ''\$'' als Endzeichen an die Eingabe gehängt. Es gilt $\$\not\in\Sigma$.
\subsection{Inverse Transformation}
Für die Invertierung einer Ausgabe $w\in\Sigma^*$ und dessen lexikograpisch sortierte Permutation $w'$ der Transformation wird zunächst eine Hilfspermutation definiert.
\begin{equation}
\begin{split}
    \gamma:\mathbb{N}\rightarrow\mathbb{N},~w_{\gamma(x)}=w'_x
\end{split}
\end{equation}
$\gamma$ bildet die Position eines Zeichens in $w$ auf dessen Position in $w'$ ab. Ein Zeichen wird also auf dessen neue Position abgebildet, nachdem $w$ sortiert wurde.

Man beobachte: 
\begin{equation}
    \gamma~\text{besteht aus einem einzigen Zyklus der Länge}~n+1.
\end{equation}
\\[.5cm]
Nun lässt sich die Inversion von BWT folgendermaßen definieren.
\begin{equation}
    w^{\text{BWT}^{-1}}:=(w_{\gamma(\$)},w_{\gamma(\gamma(\$)),..})^R
\end{equation}
\subsection{Implementierung}
\subsubsection{Transformation}
\lstinputlisting{../presentation/pseudo/bwt.pseudo}
\subsubsection{Invers}
\lstinputlisting{../presentation/pseudo/bwt-back.pseudo}
\newpage
\section{Ergebnisse}
\subsection{Einfluss der Schritte auf die Kompression}
\printbibliography[heading=bibintoc]
\end{document}